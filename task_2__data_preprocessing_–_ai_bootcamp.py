# -*- coding: utf-8 -*-
"""Task 2_ Data Preprocessing – AI Bootcamp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aeYA1BtTLMY0TSEl8RjZoUGvlFaFaeQw

# Task 2: Data Preprocessing for Machine Learning – AI Bootcamp

Download Titanic Dataset here: https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv

#### About this file

The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone on board, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).

## Section 1: Data Loading & Exploration

### **Task 1**: Load and Inspect a Dataset

*Instruction*: Load the `titanic.csv` dataset and display the first 5 rows. Show basic info and describe statistics of the dataset.
"""

import pandas as pd

df = pd.read_csv('titanic.csv')
print(df.head())
print(df.info())
print(df.describe())

"""## Section 2: Handling Missing Values

### **Task 2**: Identify and Handle Missing Data

*Instruction*:



*   Display the number of missing values per column.
*   Fill missing `Age` values with the median.
*   Drop the second row in the dataset.
"""

import pandas as pd

# Sample DataFrame with missing values
data = {
    'Sex': ['male', 'female', 'female', 'male'],
    'Pclass': [1, 3, 2, 1],
    'Age': [22, None, 24, None]
}

df = pd.DataFrame(data)

# 1. Display the number of missing values per column
print("Missing values per column:")
print(df.isnull().sum())

# 2. Fill missing Age values with the median
df['Age'] = df['Age'].fillna(df['Age'].median())

# 3. Drop the second row (index 1)
df = df.drop(index=1)

# Display the modified DataFrame
print("\nModified DataFrame:")
print(df)

"""## Section 3: Encoding Categorical Features

### **Task 3**: Convert Categorical to Numeric

*Instruction*: Convert `Sex` and `Pclass` columns to numeric using:


*   Label Encoding for `Sex`
*   One-Hot Encoding for `Pclass`
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Sample DataFrame
df = pd.DataFrame({
    'Sex': ['male', 'female', 'female', 'male'],
    'Pclass': [1, 3, 2, 1]
})

# Label Encoding for 'Sex' (male -> 1, female -> 0)
label_encoder = LabelEncoder()
df['Sex'] = label_encoder.fit_transform(df['Sex'])

# One-Hot Encoding for 'Pclass'
df = pd.get_dummies(df, columns=['Pclass'], prefix='Pclass')

# Display the transformed DataFrame
print(df)

"""## Section 4: Feature Scaling

### **Task 4**: Scale Numerical Features

*Instruction*: Use StandardScaler to scale the Age and Fare columns.*italicized text*
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Sample DataFrame with 'Age' and 'Fare' columns
data = {
    'Sex': ['male', 'female', 'female', 'male'],
    'Pclass': [1, 3, 2, 1],
    'Age': [22, 28, 24, 35],
    'Fare': [7.25, 71.2833, 7.925, 53.1]
}

df = pd.DataFrame(data)

# Initialize StandardScaler
scaler = StandardScaler()

# Scale the 'Age' and 'Fare' columns
df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])

# Display the scaled DataFrame
print("Scaled DataFrame:")
print(df)

"""## Section 5: Feature Engineering

### **Task 5**: Build Preprocessing Pipeline

*Instruction*: Using `ColumnTransformer` and `Pipeline` from `sklearn`, build a pipeline that:



*   Imputes missing values
*   Scales numeric data
*   Encodes categorical data
"""

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split

# Sample DataFrame with missing values and both numerical and categorical columns
data = {
    'Sex': ['male', 'female', 'female', 'male', None],
    'Pclass': [1, 3, 2, 1, 2],
    'Age': [22, None, 24, 35, None],
    'Fare': [7.25, 71.2833, 7.925, 53.1, None]
}

df = pd.DataFrame(data)

# Split features and target (for the sake of this example, let's assume we want to predict 'Fare')
X = df.drop('Fare', axis=1)  # Features
y = df['Fare']               # Target

# Define preprocessing steps for both categorical and numerical data
# Removed 'Fare' from numeric_features as it's the target variable
numeric_features = ['Age']
categorical_features = ['Sex', 'Pclass']

# Create the column transformer
preprocessor = ColumnTransformer(
    transformers=[
        # Impute missing values for numerical columns and scale them
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean
            ('scaler', StandardScaler())                 # Scale the data
        ]), numeric_features),

        # Impute missing values for categorical columns and encode them
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value
            ('encoder', OneHotEncoder(handle_unknown='ignore'))    # One-hot encode categorical features
        ]), categorical_features)
    ])

# Build the full pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor)
])

# Apply the transformations and create the processed DataFrame
X_processed = pipeline.fit_transform(X)

# Show the processed output
print("Transformed Features:")
print(X_processed)

# To see the column names after one-hot encoding
import numpy as np
columns = numeric_features + list(pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['encoder'].get_feature_names_out(categorical_features))
print("\nColumns after transformation:")
print(columns)

"""## Section 6: Feature Engineering

### **Task 6**: Create a New Feature

*Instruction*: Create a new feature `FamilySize` = `Siblings/Spouses Aboard` + `Parents/Children Aboard` + 1.
"""

import pandas as pd

# Sample DataFrame with 'SibSp' (Siblings/Spouses) and 'Parch' (Parents/Children) columns
data = {
    'SibSp': [1, 0, 3, 1, 0],  # Number of siblings or spouses aboard
    'Parch': [0, 1, 1, 0, 0],  # Number of parents or children aboard
}

df = pd.DataFrame(data)

# Create the new 'FamilySize' feature
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

# Display the DataFrame with the new feature
print(df)